mlpack lib needs armadillo, cereal, and ensmallen
check here for mlpack dependencies: https://github.com/mlpack/mlpack

CMAKE script:
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/libs/mlpack-4.3.0/src)
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/libs/ensmallen-2.21.0/include)
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/libs/cereal-1.3.2/include)
include_directories(${CMAKE_CURRENT_SOURCE_DIR}/libs/armadillo-12.6.7/include)

The code for the RL algorithm should be clear
get observation
done = false
do action from actor with observation (time step should be after action
while not done
    *step* time (critical line)
    get new observation
    do action from actor with new observation

g++ -Wall -O2 -I /usr/include/eigen3 -pthread -o "%e" "%f" "agent.cpp" "environment.cpp" -lpigpio -lrt

#include <Eigen/Dense>

template <typename Scalar>
class LayerNorm {
public:
  LayerNorm(int input_size) : gamma_(input_size), beta_(input_size) {
    gamma_.setOnes();
    beta_.setZero();
  }

  // Normalize a single data point
  Eigen::VectorXf normalize(const Eigen::VectorXf& input) {
    Eigen::VectorXf mean = input.array().mean();
    Eigen::VectorXf stddev = (input.array() - mean).array().sqrt().cwiseMax(1e-5);
    return gamma_.array() * (input.array() - mean).array() / stddev.array() + beta_.array();
  }

  // Normalize a matrix of data points (column-wise normalization)
  Eigen::MatrixXf normalize(const Eigen::MatrixXf& input) {
    int num_rows = input.rows();
    Eigen::MatrixXf normalized(num_rows, input.cols());
    for (int i = 0; i < num_rows; ++i) {
      normalized.row(i) = normalize(input.row(i));
    }
    return normalized;
  }

private:
  Eigen::VectorXf gamma_;
  Eigen::VectorXf beta_;
};

(from paper layer normalization)
class LayerNormalization(nn.Module):

    def __init__(self,
                 normal_shape,
                 gamma=True,
                 beta=True,
                 epsilon=1e-10):
        """Layer normalization layer

        See: [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)

        :param normal_shape: The shape of the input tensor or the last dimension of the input tensor.
        :param gamma: Add a scale parameter if it is True.
        :param beta: Add an offset parameter if it is True.
        :param epsilon: Epsilon for calculating variance.
        """
        super(LayerNormalization, self).__init__()
        if isinstance(normal_shape, int):
            normal_shape = (normal_shape,)
        else:
            normal_shape = (normal_shape[-1],)
        self.normal_shape = torch.Size(normal_shape)
        self.epsilon = epsilon
        if gamma:
            self.gamma = nn.Parameter(torch.Tensor(*normal_shape))
        else:
            self.register_parameter('gamma', None)
        if beta:
            self.beta = nn.Parameter(torch.Tensor(*normal_shape))
        else:
            self.register_parameter('beta', None)
        self.reset_parameters()

    def reset_parameters(self):
        if self.gamma is not None:
            self.gamma.data.fill_(1)
        if self.beta is not None:
            self.beta.data.zero_()

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)
        std = (var + self.epsilon).sqrt()
        y = (x - mean) / std
        if self.gamma is not None:
            y *= self.gamma
        if self.beta is not None:
            y += self.beta
        return y

    def extra_repr(self):
        return 'normal_shape={}, gamma={}, beta={}, epsilon={}'.format(
            self.normal_shape, self.gamma is not None, self.beta is not None, self.epsilon,

